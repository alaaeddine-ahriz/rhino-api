"""
Module d'Ã©valuation des rÃ©ponses des Ã©tudiants
"""
import requests
import logging
from typing import Optional, Dict, Any
from utils import load_conversations, save_conversations

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Configuration de l'API
API_BASE_URL = "http://127.0.0.1:8000/api"

class EvaluationError(Exception):
    """Exception levÃ©e en cas d'erreur lors de l'Ã©valuation"""
    pass

def evaluate_response_with_ai(question: str, response: str, context: Optional[str] = None) -> Dict[str, Any]:
    """
    Ã‰value une rÃ©ponse d'Ã©tudiant en utilisant l'IA
    
    Args:
        question: La question posÃ©e
        response: La rÃ©ponse de l'Ã©tudiant
        context: Contexte additionnel (matiÃ¨re, rÃ©fÃ©rence, etc.)
    
    Returns:
        Dict contenant l'Ã©valuation
    
    Raises:
        EvaluationError: En cas d'erreur lors de l'Ã©valuation
    """
    try:
        url = f"{API_BASE_URL}/evaluations/evaluate"
        payload = {
            "question": question,
            "response": response,
            "context": context or ""
        }
        
        logger.info(f"Envoi de l'Ã©valuation Ã  l'API...")
        response_api = requests.post(url, json=payload, timeout=30)
        response_api.raise_for_status()
        
        data = response_api.json()
        
        if not data.get("success"):
            raise EvaluationError(f"API Error: {data.get('message', 'Erreur inconnue')}")
        
        evaluation = data.get("data", {})
        return {
            "score": evaluation.get("score", 0),
            "feedback": evaluation.get("feedback", ""),
            "points_positifs": evaluation.get("points_positifs", []),
            "points_amelioration": evaluation.get("points_amelioration", []),
            "note_sur_20": evaluation.get("note_sur_20", 0)
        }
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Erreur de connexion Ã  l'API d'Ã©valuation: {e}")
        raise EvaluationError(f"Impossible de se connecter Ã  l'API: {e}")
    except Exception as e:
        logger.error(f"Erreur lors de l'Ã©valuation: {e}")
        raise EvaluationError(f"Erreur inattendue: {e}")

def evaluate_pending_responses() -> int:
    """
    Ã‰value toutes les rÃ©ponses en attente
    
    Returns:
        Nombre de rÃ©ponses Ã©valuÃ©es
    """
    conversations = load_conversations()
    evaluated_count = 0
    
    for question_id, conv in conversations.items():
        # VÃ©rifier si la rÃ©ponse existe et n'est pas dÃ©jÃ  Ã©valuÃ©e
        if conv.get("response") and not conv.get("evaluated"):
            try:
                logger.info(f"Ã‰valuation de la rÃ©ponse pour l'ID {question_id}")
                
                # PrÃ©parer le contexte
                context = f"MatiÃ¨re: {conv.get('matiere', 'N/A')}, RÃ©fÃ©rence: {conv.get('challenge_ref', 'N/A')}"
                
                # Ã‰valuer la rÃ©ponse
                evaluation = evaluate_response_with_ai(
                    question=conv["question"],
                    response=conv["response"],
                    context=context
                )
                
                # Mettre Ã  jour la conversation
                conversations[question_id]["evaluation"] = evaluation
                conversations[question_id]["evaluated"] = True
                
                logger.info(f"âœ… RÃ©ponse Ã©valuÃ©e pour {conv['student']}")
                logger.info(f"   - Note: {evaluation['note_sur_20']}/20")
                logger.info(f"   - Score: {evaluation['score']}%")
                
                evaluated_count += 1
                
            except EvaluationError as e:
                logger.error(f"âŒ Erreur d'Ã©valuation pour l'ID {question_id}: {e}")
                continue
            except Exception as e:
                logger.error(f"âŒ Erreur inattendue pour l'ID {question_id}: {e}")
                continue
    
    # Sauvegarder les modifications
    if evaluated_count > 0:
        save_conversations(conversations)
        logger.info(f"ğŸ’¾ {evaluated_count} Ã©valuation(s) sauvegardÃ©e(s)")
    
    return evaluated_count

def get_evaluation_report() -> Dict[str, Any]:
    """
    GÃ©nÃ¨re un rapport d'Ã©valuation
    
    Returns:
        Dict contenant les statistiques d'Ã©valuation
    """
    conversations = load_conversations()
    
    total_responses = 0
    evaluated_responses = 0
    pending_responses = 0
    total_score = 0
    scores = []
    
    for conv in conversations.values():
        if conv.get("response"):
            total_responses += 1
            
            if conv.get("evaluated"):
                evaluated_responses += 1
                evaluation = conv.get("evaluation", {})
                score = evaluation.get("note_sur_20", 0)
                scores.append(score)
                total_score += score
            else:
                pending_responses += 1
    
    average_score = total_score / evaluated_responses if evaluated_responses > 0 else 0
    
    return {
        "total_responses": total_responses,
        "evaluated_responses": evaluated_responses,
        "pending_responses": pending_responses,
        "average_score": round(average_score, 2),
        "min_score": min(scores) if scores else 0,
        "max_score": max(scores) if scores else 0,
        "all_scores": scores
    }

def print_evaluation_report():
    """Affiche un rapport d'Ã©valuation formatÃ©"""
    report = get_evaluation_report()
    
    print("\n" + "="*50)
    print("ğŸ“Š RAPPORT D'Ã‰VALUATION")
    print("="*50)
    print(f"ğŸ“¨ Total des rÃ©ponses: {report['total_responses']}")
    print(f"âœ… RÃ©ponses Ã©valuÃ©es: {report['evaluated_responses']}")
    print(f"â³ RÃ©ponses en attente: {report['pending_responses']}")
    
    if report['evaluated_responses'] > 0:
        print(f"ğŸ“ˆ Note moyenne: {report['average_score']}/20")
        print(f"ğŸ† Meilleure note: {report['max_score']}/20")
        print(f"ğŸ“‰ Note la plus basse: {report['min_score']}/20")
    
    print("="*50)

def test_evaluation_api() -> bool:
    """Teste la connexion Ã  l'API d'Ã©valuation"""
    try:
        response = requests.get(f"{API_BASE_URL}/evaluations/", timeout=5)
        return response.status_code in [200, 404]  # 404 peut Ãªtre normal si l'endpoint n'existe pas encore
    except:
        return False

if __name__ == "__main__":
    # Test de connexion API
    if test_evaluation_api():
        logger.info("âœ… Connexion API d'Ã©valuation OK")
    else:
        logger.warning("âš ï¸ API d'Ã©valuation non disponible")
    
    # Ã‰valuer les rÃ©ponses en attente
    count = evaluate_pending_responses()
    
    if count > 0:
        logger.info(f"ğŸ‰ {count} rÃ©ponse(s) Ã©valuÃ©e(s) avec succÃ¨s!")
        print_evaluation_report()
    else:
        logger.info("â„¹ï¸ Aucune rÃ©ponse en attente d'Ã©valuation")
        print_evaluation_report() 