#!/usr/bin/env python3
"""
Simple response evaluation functionality
"""

import logging
from typing import Dict, Optional

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

def evaluate_response_simple(question: str, response: str, matiere: str) -> Dict:
    """
    Ã‰valuation simple d'une rÃ©ponse basÃ©e sur des critÃ¨res de base
    
    Args:
        question: La question posÃ©e
        response: La rÃ©ponse de l'Ã©tudiant
        matiere: La matiÃ¨re concernÃ©e
    
    Returns:
        Dict contenant l'Ã©valuation
    """
    evaluation = {
        'score': 0,
        'max_score': 100,
        'feedback': [],
        'grade': 'F',
        'details': {}
    }
    
    # CritÃ¨res de base
    response_length = len(response.strip())
    word_count = len(response.split())
    
    # CritÃ¨re 1: Longueur de la rÃ©ponse (20 points max)
    if response_length < 50:
        evaluation['feedback'].append("âŒ RÃ©ponse trop courte (moins de 50 caractÃ¨res)")
        length_score = 0
    elif response_length < 200:
        evaluation['feedback'].append("âš ï¸ RÃ©ponse courte mais acceptable")
        length_score = 10
    elif response_length < 500:
        evaluation['feedback'].append("âœ… Longueur de rÃ©ponse appropriÃ©e")
        length_score = 20
    else:
        evaluation['feedback'].append("âœ… RÃ©ponse dÃ©taillÃ©e")
        length_score = 20
    
    # CritÃ¨re 2: Nombre de mots (15 points max)
    if word_count < 10:
        evaluation['feedback'].append("âŒ RÃ©ponse trop brÃ¨ve (moins de 10 mots)")
        word_score = 0
    elif word_count < 50:
        evaluation['feedback'].append("âš ï¸ RÃ©ponse succincte")
        word_score = 8
    else:
        evaluation['feedback'].append("âœ… RÃ©ponse dÃ©veloppÃ©e")
        word_score = 15
    
    # CritÃ¨re 3: PrÃ©sence de mots-clÃ©s selon la matiÃ¨re (25 points max)
    keyword_score = check_keywords_by_subject(response, matiere)
    
    # CritÃ¨re 4: Structure et prÃ©sentation (20 points max)
    structure_score = check_structure(response)
    
    # CritÃ¨re 5: Effort apparent (20 points max)
    effort_score = check_effort(response)
    
    # Calcul du score total
    total_score = length_score + word_score + keyword_score + structure_score + effort_score
    evaluation['score'] = min(total_score, 100)
    
    # Attribution de la note
    if evaluation['score'] >= 90:
        evaluation['grade'] = 'A+'
    elif evaluation['score'] >= 80:
        evaluation['grade'] = 'A'
    elif evaluation['score'] >= 70:
        evaluation['grade'] = 'B'
    elif evaluation['score'] >= 60:
        evaluation['grade'] = 'C'
    elif evaluation['score'] >= 50:
        evaluation['grade'] = 'D'
    else:
        evaluation['grade'] = 'F'
    
    # DÃ©tails des scores
    evaluation['details'] = {
        'length_score': length_score,
        'word_score': word_score,
        'keyword_score': keyword_score,
        'structure_score': structure_score,
        'effort_score': effort_score,
        'response_length': response_length,
        'word_count': word_count
    }
    
    return evaluation

def check_keywords_by_subject(response: str, matiere: str) -> int:
    """VÃ©rifie la prÃ©sence de mots-clÃ©s spÃ©cifiques Ã  la matiÃ¨re"""
    response_lower = response.lower()
    score = 0
    
    keywords = {
        'SYD': ['systÃ¨me', 'distribuÃ©', 'rÃ©seau', 'serveur', 'client', 'protocole', 'tcp', 'udp', 'http', 'consensus', 'raft', 'byzantine', 'cohÃ©rence', 'disponibilitÃ©', 'partition'],
        'TCP': ['tcp', 'protocole', 'transport', 'fiable', 'connexion', 'segment', 'port', 'socket', 'flow control', 'congestion', 'window', 'acknowledgment', 'handshake', 'syn', 'ack'],
        'MATH': ['Ã©quation', 'fonction', 'dÃ©rivÃ©e', 'intÃ©grale', 'limite', 'calcul', 'mathÃ©matique', 'formule', 'variable', 'constante'],
        'PHYS': ['force', 'Ã©nergie', 'masse', 'vitesse', 'accÃ©lÃ©ration', 'newton', 'joule', 'physique', 'gravitÃ©', 'Ã©lectricitÃ©'],
        'INFO': ['algorithme', 'programmation', 'code', 'variable', 'fonction', 'boucle', 'condition', 'donnÃ©es', 'structure', 'informatique']
    }
    
    subject_keywords = keywords.get(matiere, [])
    
    found_keywords = []
    for keyword in subject_keywords:
        if keyword in response_lower:
            found_keywords.append(keyword)
            score += 2  # 2 points par mot-clÃ© trouvÃ©
    
    if found_keywords:
        feedback_msg = f"âœ… Mots-clÃ©s pertinents trouvÃ©s: {', '.join(found_keywords)}"
    else:
        feedback_msg = f"âŒ Aucun mot-clÃ© spÃ©cifique Ã  {matiere} trouvÃ©"
    
    return min(score, 25)  # Maximum 25 points

def check_structure(response: str) -> int:
    """VÃ©rifie la structure de la rÃ©ponse"""
    score = 0
    feedback = []
    
    # PrÃ©sence de phrases complÃ¨tes
    sentences = response.split('.')
    if len(sentences) > 2:
        score += 8
        feedback.append("âœ… RÃ©ponse structurÃ©e en phrases")
    
    # PrÃ©sence de paragraphes
    paragraphs = response.split('\n\n')
    if len(paragraphs) > 1:
        score += 6
        feedback.append("âœ… RÃ©ponse organisÃ©e en paragraphes")
    
    # Utilisation de majuscules
    if any(c.isupper() for c in response):
        score += 3
        feedback.append("âœ… Utilisation correcte des majuscules")
    
    # Ponctuation appropriÃ©e
    if any(p in response for p in ['.', '!', '?', ',']):
        score += 3
        feedback.append("âœ… Ponctuation prÃ©sente")
    
    return min(score, 20)

def check_effort(response: str) -> int:
    """Ã‰value l'effort apparent dans la rÃ©ponse"""
    score = 0
    
    # VariÃ©tÃ© du vocabulaire
    words = response.lower().split()
    unique_words = set(words)
    vocabulary_ratio = len(unique_words) / len(words) if words else 0
    
    if vocabulary_ratio > 0.7:
        score += 10  # Vocabulaire variÃ©
    elif vocabulary_ratio > 0.5:
        score += 6   # Vocabulaire correct
    else:
        score += 2   # Vocabulaire rÃ©pÃ©titif
    
    # PrÃ©sence d'exemples ou d'explications
    if any(indicator in response.lower() for indicator in ['exemple', 'par exemple', 'comme', 'c\'est-Ã -dire', 'notamment']):
        score += 5
    
    # Effort de rÃ©flexion apparent
    if any(indicator in response.lower() for indicator in ['parce que', 'car', 'donc', 'ainsi', 'cependant', 'nÃ©anmoins']):
        score += 5
    
    return min(score, 20)

def display_evaluation(evaluation: Dict, question: str, response: str):
    """Affiche l'Ã©valuation de maniÃ¨re formatÃ©e"""
    print("\n" + "ğŸ“Š" * 30)
    print("Ã‰VALUATION DE LA RÃ‰PONSE")
    print("ğŸ“Š" * 30)
    
    print(f"ğŸ“ Question: {question[:100]}...")
    print(f"ğŸ¯ Score: {evaluation['score']}/{evaluation['max_score']}")
    print(f"ğŸ“Š Note: {evaluation['grade']}")
    
    print("\nğŸ“‹ Feedback dÃ©taillÃ©:")
    for feedback in evaluation['feedback']:
        print(f"   {feedback}")
    
    print(f"\nğŸ“ˆ DÃ©tail des scores:")
    details = evaluation['details']
    print(f"   â€¢ Longueur: {details['length_score']}/20 ({details['response_length']} caractÃ¨res)")
    print(f"   â€¢ Nombre de mots: {details['word_score']}/15 ({details['word_count']} mots)")
    print(f"   â€¢ Mots-clÃ©s: {details['keyword_score']}/25")
    print(f"   â€¢ Structure: {details['structure_score']}/20")
    print(f"   â€¢ Effort: {details['effort_score']}/20")
    
    print("\nğŸ’¡ Recommandations:")
    if evaluation['score'] < 60:
        print("   â€¢ DÃ©velopper davantage la rÃ©ponse")
        print("   â€¢ Utiliser des termes techniques appropriÃ©s")
        print("   â€¢ Structurer la rÃ©ponse en paragraphes")
    elif evaluation['score'] < 80:
        print("   â€¢ Ajouter plus d'exemples concrets")
        print("   â€¢ Approfondir l'explication")
    else:
        print("   â€¢ Excellente rÃ©ponse, continuez ainsi!")

def evaluate_and_display(question: str, response: str, matiere: str) -> Dict:
    """Ã‰value et affiche une rÃ©ponse"""
    evaluation = evaluate_response_simple(question, response, matiere)
    display_evaluation(evaluation, question, response)
    return evaluation

def send_feedback_email(to_email: str, evaluation: Dict, question: str, response: str, student_name: str = None, original_email: Dict = None) -> bool:
    """
    Envoie un email de feedback avec l'Ã©valuation Ã  l'Ã©tudiant en rÃ©ponse Ã  son email
    
    Args:
        to_email: Adresse email de l'Ã©tudiant
        evaluation: Dictionnaire contenant l'Ã©valuation
        question: Question originale
        response: RÃ©ponse de l'Ã©tudiant
        student_name: Nom de l'Ã©tudiant (optionnel)
        original_email: Dict contenant les infos de l'email original pour crÃ©er une rÃ©ponse
    
    Returns:
        bool: True si envoyÃ© avec succÃ¨s
    """
    try:
        import yagmail
        from config import EMAIL, PASSWORD
        
        # PrÃ©parer le contenu du feedback
        student_greeting = f"Bonjour {student_name}" if student_name else "Bonjour"
        
        # PrÃ©parer le sujet en rÃ©ponse Ã  l'email original
        if original_email and original_email.get('subject'):
            original_subject = original_email['subject']
            # Supprimer les "Re: " existants pour Ã©viter "Re: Re: ..."
            clean_subject = original_subject
            while clean_subject.startswith('Re: ') or clean_subject.startswith('RE: '):
                clean_subject = clean_subject[4:]
            subject = f"Re: {clean_subject} - ğŸ“Š Note: {evaluation['grade']} ({evaluation['score']}/100)"
        else:
            subject = f"ğŸ“Š Feedback - Note: {evaluation['grade']} ({evaluation['score']}/100)"
        
        # Corps du message de feedback
        body = f"""{student_greeting},

Voici l'Ã©valuation de votre rÃ©ponse :

ğŸ¯ **RÃ‰SULTAT GLOBAL**
â€¢ Score : {evaluation['score']}/100
â€¢ Note : {evaluation['grade']}

ğŸ“ **QUESTION POSÃ‰E**
{question}

ğŸ“„ **VOTRE RÃ‰PONSE**
{response[:200]}{'...' if len(response) > 200 else ''}

ğŸ“Š **DÃ‰TAIL DE L'Ã‰VALUATION**
{format_evaluation_details(evaluation)}

ğŸ“‹ **FEEDBACK DÃ‰TAILLÃ‰**
{format_feedback_list(evaluation['feedback'])}

ğŸ’¡ **RECOMMANDATIONS**
{format_recommendations(evaluation['score'])}

{format_encouragement(evaluation['grade'])}

Cordialement,
Le systÃ¨me d'Ã©valuation automatique ğŸ¤–
"""
        
        # Envoi de l'email avec en-tÃªtes de rÃ©ponse si disponibles
        logger.info(f"Envoi du feedback Ã  {to_email}")
        yag = yagmail.SMTP(EMAIL, PASSWORD)
        
        # PrÃ©parer les en-tÃªtes pour crÃ©er une rÃ©ponse dans le mÃªme thread
        headers = {}
        if original_email:
            # Extraire le Message-ID de l'email original
            original_message_id = original_email.get('message_id')
            if original_message_id:
                headers['In-Reply-To'] = original_message_id
                headers['References'] = original_message_id
                logger.info(f"Envoi en rÃ©ponse au message ID: {original_message_id}")
        
        if headers:
            # Envoyer avec en-tÃªtes personnalisÃ©s pour crÃ©er une rÃ©ponse
            yag.send(to=to_email, subject=subject, contents=body, headers=headers)
        else:
            # Envoi normal si pas d'informations pour la rÃ©ponse
            yag.send(to=to_email, subject=subject, contents=body)
        
        logger.info(f"âœ… Feedback envoyÃ© avec succÃ¨s Ã  {to_email}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Erreur envoi feedback: {e}")
        return False

def format_evaluation_details(evaluation: Dict) -> str:
    """Formate les dÃ©tails de l'Ã©valuation pour l'email"""
    details = evaluation['details']
    return f"""â€¢ Longueur de rÃ©ponse : {details['length_score']}/20 points ({details['response_length']} caractÃ¨res)
â€¢ Nombre de mots : {details['word_score']}/15 points ({details['word_count']} mots)
â€¢ Mots-clÃ©s techniques : {details['keyword_score']}/25 points
â€¢ Structure et prÃ©sentation : {details['structure_score']}/20 points
â€¢ Effort et rÃ©flexion : {details['effort_score']}/20 points"""

def format_feedback_list(feedback_list: list) -> str:
    """Formate la liste de feedback pour l'email"""
    return '\n'.join([f"â€¢ {feedback}" for feedback in feedback_list])

def format_recommendations(score: int) -> str:
    """GÃ©nÃ¨re des recommandations basÃ©es sur le score"""
    if score < 60:
        return """â€¢ DÃ©veloppez davantage votre rÃ©ponse pour montrer votre comprÃ©hension
â€¢ Utilisez des termes techniques appropriÃ©s Ã  la matiÃ¨re
â€¢ Structurez votre rÃ©ponse en paragraphes clairs
â€¢ Ajoutez des exemples concrets pour illustrer vos propos"""
    elif score < 80:
        return """â€¢ Bonne base ! Ajoutez plus d'exemples concrets
â€¢ Approfondissez certains aspects de votre explication
â€¢ Utilisez plus de vocabulaire technique spÃ©cialisÃ©"""
    else:
        return """â€¢ Excellente rÃ©ponse ! Continuez sur cette lancÃ©e
â€¢ Votre maÃ®trise du sujet est Ã©vidente
â€¢ La structure et le contenu sont trÃ¨s satisfaisants"""

def format_encouragement(grade: str) -> str:
    """GÃ©nÃ¨re un message d'encouragement basÃ© sur la note"""
    encouragements = {
        'A+': "ğŸŒŸ Travail exceptionnel ! Vous maÃ®trisez parfaitement le sujet.",
        'A': "ğŸ‰ TrÃ¨s bon travail ! Vous dÃ©montrez une solide comprÃ©hension.",
        'B': "ğŸ‘ Bon travail ! Continuez vos efforts, vous Ãªtes sur la bonne voie.",
        'C': "ğŸ’ª Travail correct. Avec un peu plus d'effort, vous pouvez encore progresser.",
        'D': "ğŸ“š Il y a des amÃ©liorations Ã  apporter. N'hÃ©sitez pas Ã  approfondir vos rÃ©visions.",
        'F': "ğŸ”„ Cette rÃ©ponse nÃ©cessite plus de travail. Reprenez les concepts de base et n'hÃ©sitez pas Ã  demander de l'aide."
    }
    return encouragements.get(grade, "Continuez vos efforts !")

def evaluate_display_and_send_feedback(question: str, response: str, matiere: str, 
                                      student_email: str, student_name: str = None, original_email: Dict = None) -> tuple:
    """
    Ã‰value une rÃ©ponse, l'affiche et envoie le feedback par email
    
    Returns:
        tuple: (evaluation_dict, feedback_sent_success)
    """
    # Ã‰valuer et afficher
    evaluation = evaluate_and_display(question, response, matiere)
    
    # Envoyer le feedback
    feedback_sent = send_feedback_email(student_email, evaluation, question, response, student_name, original_email)
    
    return evaluation, feedback_sent 