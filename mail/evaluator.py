#!/usr/bin/env python3
"""
Simple response evaluation functionality
"""

import logging
from typing import Dict, Optional

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

def evaluate_response_simple(question: str, response: str, matiere: str) -> Dict:
    """
    Ã‰valuation simple d'une rÃ©ponse basÃ©e sur des critÃ¨res de base
    
    Args:
        question: La question posÃ©e
        response: La rÃ©ponse de l'Ã©tudiant
        matiere: La matiÃ¨re concernÃ©e
    
    Returns:
        Dict contenant l'Ã©valuation
    """
    evaluation = {
        'score': 0,
        'max_score': 100,
        'feedback': [],
        'grade': 'F',
        'details': {}
    }
    
    # CritÃ¨res de base
    response_length = len(response.strip())
    word_count = len(response.split())
    
    # CritÃ¨re 1: Longueur de la rÃ©ponse (20 points max)
    if response_length < 50:
        evaluation['feedback'].append("âŒ RÃ©ponse trop courte (moins de 50 caractÃ¨res)")
        length_score = 0
    elif response_length < 200:
        evaluation['feedback'].append("âš ï¸ RÃ©ponse courte mais acceptable")
        length_score = 10
    elif response_length < 500:
        evaluation['feedback'].append("âœ… Longueur de rÃ©ponse appropriÃ©e")
        length_score = 20
    else:
        evaluation['feedback'].append("âœ… RÃ©ponse dÃ©taillÃ©e")
        length_score = 20
    
    # CritÃ¨re 2: Nombre de mots (15 points max)
    if word_count < 10:
        evaluation['feedback'].append("âŒ RÃ©ponse trop brÃ¨ve (moins de 10 mots)")
        word_score = 0
    elif word_count < 50:
        evaluation['feedback'].append("âš ï¸ RÃ©ponse succincte")
        word_score = 8
    else:
        evaluation['feedback'].append("âœ… RÃ©ponse dÃ©veloppÃ©e")
        word_score = 15
    
    # CritÃ¨re 3: PrÃ©sence de mots-clÃ©s selon la matiÃ¨re (25 points max)
    keyword_score = check_keywords_by_subject(response, matiere)
    
    # CritÃ¨re 4: Structure et prÃ©sentation (20 points max)
    structure_score = check_structure(response)
    
    # CritÃ¨re 5: Effort apparent (20 points max)
    effort_score = check_effort(response)
    
    # Calcul du score total
    total_score = length_score + word_score + keyword_score + structure_score + effort_score
    evaluation['score'] = min(total_score, 100)
    
    # Attribution de la note
    if evaluation['score'] >= 90:
        evaluation['grade'] = 'A+'
    elif evaluation['score'] >= 80:
        evaluation['grade'] = 'A'
    elif evaluation['score'] >= 70:
        evaluation['grade'] = 'B'
    elif evaluation['score'] >= 60:
        evaluation['grade'] = 'C'
    elif evaluation['score'] >= 50:
        evaluation['grade'] = 'D'
    else:
        evaluation['grade'] = 'F'
    
    # DÃ©tails des scores
    evaluation['details'] = {
        'length_score': length_score,
        'word_score': word_score,
        'keyword_score': keyword_score,
        'structure_score': structure_score,
        'effort_score': effort_score,
        'response_length': response_length,
        'word_count': word_count
    }
    
    return evaluation

def check_keywords_by_subject(response: str, matiere: str) -> int:
    """VÃ©rifie la prÃ©sence de mots-clÃ©s spÃ©cifiques Ã  la matiÃ¨re"""
    response_lower = response.lower()
    score = 0
    
    keywords = {
        'SYD': ['systÃ¨me', 'distribuÃ©', 'rÃ©seau', 'serveur', 'client', 'protocole', 'tcp', 'udp', 'http', 'consensus', 'raft', 'byzantine', 'cohÃ©rence', 'disponibilitÃ©', 'partition'],
        'TCP': ['tcp', 'protocole', 'transport', 'fiable', 'connexion', 'segment', 'port', 'socket', 'flow control', 'congestion', 'window', 'acknowledgment', 'handshake', 'syn', 'ack'],
        'MATH': ['Ã©quation', 'fonction', 'dÃ©rivÃ©e', 'intÃ©grale', 'limite', 'calcul', 'mathÃ©matique', 'formule', 'variable', 'constante'],
        'PHYS': ['force', 'Ã©nergie', 'masse', 'vitesse', 'accÃ©lÃ©ration', 'newton', 'joule', 'physique', 'gravitÃ©', 'Ã©lectricitÃ©'],
        'INFO': ['algorithme', 'programmation', 'code', 'variable', 'fonction', 'boucle', 'condition', 'donnÃ©es', 'structure', 'informatique']
    }
    
    subject_keywords = keywords.get(matiere, [])
    
    found_keywords = []
    for keyword in subject_keywords:
        if keyword in response_lower:
            found_keywords.append(keyword)
            score += 2  # 2 points par mot-clÃ© trouvÃ©
    
    if found_keywords:
        feedback_msg = f"âœ… Mots-clÃ©s pertinents trouvÃ©s: {', '.join(found_keywords)}"
    else:
        feedback_msg = f"âŒ Aucun mot-clÃ© spÃ©cifique Ã  {matiere} trouvÃ©"
    
    return min(score, 25)  # Maximum 25 points

def check_structure(response: str) -> int:
    """VÃ©rifie la structure de la rÃ©ponse"""
    score = 0
    feedback = []
    
    # PrÃ©sence de phrases complÃ¨tes
    sentences = response.split('.')
    if len(sentences) > 2:
        score += 8
        feedback.append("âœ… RÃ©ponse structurÃ©e en phrases")
    
    # PrÃ©sence de paragraphes
    paragraphs = response.split('\n\n')
    if len(paragraphs) > 1:
        score += 6
        feedback.append("âœ… RÃ©ponse organisÃ©e en paragraphes")
    
    # Utilisation de majuscules
    if any(c.isupper() for c in response):
        score += 3
        feedback.append("âœ… Utilisation correcte des majuscules")
    
    # Ponctuation appropriÃ©e
    if any(p in response for p in ['.', '!', '?', ',']):
        score += 3
        feedback.append("âœ… Ponctuation prÃ©sente")
    
    return min(score, 20)

def check_effort(response: str) -> int:
    """Ã‰value l'effort apparent dans la rÃ©ponse"""
    score = 0
    
    # VariÃ©tÃ© du vocabulaire
    words = response.lower().split()
    unique_words = set(words)
    vocabulary_ratio = len(unique_words) / len(words) if words else 0
    
    if vocabulary_ratio > 0.7:
        score += 10  # Vocabulaire variÃ©
    elif vocabulary_ratio > 0.5:
        score += 6   # Vocabulaire correct
    else:
        score += 2   # Vocabulaire rÃ©pÃ©titif
    
    # PrÃ©sence d'exemples ou d'explications
    if any(indicator in response.lower() for indicator in ['exemple', 'par exemple', 'comme', 'c\'est-Ã -dire', 'notamment']):
        score += 5
    
    # Effort de rÃ©flexion apparent
    if any(indicator in response.lower() for indicator in ['parce que', 'car', 'donc', 'ainsi', 'cependant', 'nÃ©anmoins']):
        score += 5
    
    return min(score, 20)

def display_evaluation(evaluation: Dict, question: str, response: str):
    """Affiche l'Ã©valuation de maniÃ¨re formatÃ©e"""
    print("\n" + "ğŸ“Š" * 30)
    print("Ã‰VALUATION DE LA RÃ‰PONSE")
    print("ğŸ“Š" * 30)
    
    print(f"ğŸ“ Question: {question[:100]}...")
    print(f"ğŸ¯ Score: {evaluation['score']}/{evaluation['max_score']}")
    print(f"ğŸ“Š Note: {evaluation['grade']}")
    
    print("\nğŸ“‹ Feedback dÃ©taillÃ©:")
    for feedback in evaluation['feedback']:
        print(f"   {feedback}")
    
    print(f"\nğŸ“ˆ DÃ©tail des scores:")
    details = evaluation['details']
    print(f"   â€¢ Longueur: {details['length_score']}/20 ({details['response_length']} caractÃ¨res)")
    print(f"   â€¢ Nombre de mots: {details['word_score']}/15 ({details['word_count']} mots)")
    print(f"   â€¢ Mots-clÃ©s: {details['keyword_score']}/25")
    print(f"   â€¢ Structure: {details['structure_score']}/20")
    print(f"   â€¢ Effort: {details['effort_score']}/20")
    
    print("\nğŸ’¡ Recommandations:")
    if evaluation['score'] < 60:
        print("   â€¢ DÃ©velopper davantage la rÃ©ponse")
        print("   â€¢ Utiliser des termes techniques appropriÃ©s")
        print("   â€¢ Structurer la rÃ©ponse en paragraphes")
    elif evaluation['score'] < 80:
        print("   â€¢ Ajouter plus d'exemples concrets")
        print("   â€¢ Approfondir l'explication")
    else:
        print("   â€¢ Excellente rÃ©ponse, continuez ainsi!")

def evaluate_and_display(question: str, response: str, matiere: str) -> Dict:
    """Ã‰value et affiche une rÃ©ponse"""
    evaluation = evaluate_response_simple(question, response, matiere)
    display_evaluation(evaluation, question, response)
    return evaluation 